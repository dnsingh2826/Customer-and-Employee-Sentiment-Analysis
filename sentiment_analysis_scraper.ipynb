{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making sentiment analyzer for email classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\singh\\OneDrive - Manipal University Jaipur\\code\\Python\\EY\\Scraping Codes\\sentiment_analysis_scraper.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m   <a href='vscode-notebook-cell:/c%3A/Users/singh/OneDrive%20-%20Manipal%20University%20Jaipur/code/Python/EY/Scraping%20Codes/sentiment_analysis_scraper.ipynb#W0sZmlsZQ%3D%3D?line=1247'>1248</a>\u001b[0m opt\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--incognito\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   <a href='vscode-notebook-cell:/c%3A/Users/singh/OneDrive%20-%20Manipal%20University%20Jaipur/code/Python/EY/Scraping%20Codes/sentiment_analysis_scraper.ipynb#W0sZmlsZQ%3D%3D?line=1248'>1249</a>\u001b[0m opt\u001b[39m.\u001b[39madd_argument(\u001b[39m'\u001b[39m\u001b[39m--disable-notifications\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> <a href='vscode-notebook-cell:/c%3A/Users/singh/OneDrive%20-%20Manipal%20University%20Jaipur/code/Python/EY/Scraping%20Codes/sentiment_analysis_scraper.ipynb#W0sZmlsZQ%3D%3D?line=1249'>1250</a>\u001b[0m driver \u001b[39m=\u001b[39m uc\u001b[39m.\u001b[39;49mChrome(options\u001b[39m=\u001b[39;49mopt)\n\u001b[0;32m   <a href='vscode-notebook-cell:/c%3A/Users/singh/OneDrive%20-%20Manipal%20University%20Jaipur/code/Python/EY/Scraping%20Codes/sentiment_analysis_scraper.ipynb#W0sZmlsZQ%3D%3D?line=1250'>1251</a>\u001b[0m driver\u001b[39m.\u001b[39mmaximize_window()\n\u001b[0;32m   <a href='vscode-notebook-cell:/c%3A/Users/singh/OneDrive%20-%20Manipal%20University%20Jaipur/code/Python/EY/Scraping%20Codes/sentiment_analysis_scraper.ipynb#W0sZmlsZQ%3D%3D?line=1251'>1252</a>\u001b[0m action \u001b[39m=\u001b[39m ActionChains(driver)\n",
      "File \u001b[1;32mc:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\undetected_chromedriver\\__init__.py:258\u001b[0m, in \u001b[0;36mChrome.__init__\u001b[1;34m(self, options, user_data_dir, driver_executable_path, browser_executable_path, port, enable_cdp_events, desired_capabilities, advanced_elements, keep_alive, log_level, headless, version_main, patcher_force_close, suppress_welcome, use_subprocess, debug, no_sandbox, user_multi_procs, **kw)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatcher \u001b[39m=\u001b[39m Patcher(\n\u001b[0;32m    252\u001b[0m     executable_path\u001b[39m=\u001b[39mdriver_executable_path,\n\u001b[0;32m    253\u001b[0m     force\u001b[39m=\u001b[39mpatcher_force_close,\n\u001b[0;32m    254\u001b[0m     version_main\u001b[39m=\u001b[39mversion_main,\n\u001b[0;32m    255\u001b[0m     user_multi_procs\u001b[39m=\u001b[39muser_multi_procs,\n\u001b[0;32m    256\u001b[0m )\n\u001b[0;32m    257\u001b[0m \u001b[39m# self.patcher.auto(user_multiprocess = user_multi_num_procs)\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatcher\u001b[39m.\u001b[39;49mauto()\n\u001b[0;32m    260\u001b[0m \u001b[39m# self.patcher = patcher\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options:\n",
      "File \u001b[1;32mc:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\undetected_chromedriver\\patcher.py:178\u001b[0m, in \u001b[0;36mPatcher.auto\u001b[1;34m(self, executable_path, force, version_main, _)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion_main \u001b[39m=\u001b[39m release\u001b[39m.\u001b[39mversion[\u001b[39m0\u001b[39m]\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion_full \u001b[39m=\u001b[39m release\n\u001b[1;32m--> 178\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munzip_package(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetch_package())\n\u001b[0;32m    179\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch()\n",
      "File \u001b[1;32mc:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\undetected_chromedriver\\patcher.py:287\u001b[0m, in \u001b[0;36mPatcher.fetch_package\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m     download_url \u001b[39m%\u001b[39m\u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion_full\u001b[39m.\u001b[39mvstring, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplatform_name, zip_name)\n\u001b[0;32m    286\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mdownloading from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m download_url)\n\u001b[1;32m--> 287\u001b[0m \u001b[39mreturn\u001b[39;00m urlretrieve(download_url)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[0;32m    269\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     block \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(bs)\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m block:\n\u001b[0;32m    272\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mread(amt)\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from bs4 import BeautifulSoup\n",
    "import undetected_chromedriver as uc\n",
    "from linkedin_scraper import actions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "def Indeed_scrape():\n",
    "    global df_main_indeed\n",
    "    global s\n",
    "\n",
    "    url = \"https://in.indeed.com/companies?from=gnav-acme--acme-webapp\"\n",
    "    driver.get(url)\n",
    "    main_page = driver.current_window_handle\n",
    "\n",
    "    #clicking the sign-in button\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"gnav-main-container\"]/div/div/div[2]/div[2]/div[2]/a').click()\n",
    "\n",
    "    #clicking on sign-in using google button\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"login-google-button\"]').click()\n",
    "\n",
    "    # changing windows\n",
    "    for handle in driver.window_handles:\n",
    "        if handle != main_page:\n",
    "            login_page = handle\n",
    "    driver.switch_to.window(login_page)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        enter_email = driver.find_element(By.XPATH, '//*[@id=\"identifierId\"]')\n",
    "        enter_email.send_keys(email)\n",
    "        enter_email.send_keys(Keys.RETURN)\n",
    "        time.sleep(5)\n",
    "        enter_pass = driver.find_element(By.XPATH, '//*[@id=\"password\"]/div[1]/div/div[1]/input')\n",
    "        enter_pass.send_keys(p)\n",
    "        driver.switch_to.window(login_page)\n",
    "        enter_pass.send_keys(Keys.RETURN)\n",
    "    \n",
    "    except:\n",
    "        if \"Use another account\" in driver.page_source:\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"view_container\"]/div/div/div[2]/div/div[1]/div/form/span/section/div/div/div/div/ul/li[1]/div/div[1]/div/div[2]/div[1]').click()\n",
    "            time.sleep(3)\n",
    "\n",
    "    driver.switch_to.window(main_page)\n",
    "    time.sleep(3)\n",
    "\n",
    "    if s == 'CSL Finance' or s == 'csl finance' or s == 'CSL FINANCE':\n",
    "        s = 'CSL Finance Limited'\n",
    "\n",
    "    #searching for the company\n",
    "    wait = WebDriverWait(driver, 500)\n",
    "    text_element = wait.until(EC.visibility_of_element_located((By.XPATH, \"//h1[contains(text(), 'Find great places to work')]\")))\n",
    "    driver.get(\"https://in.indeed.com/cmp/{}/reviews\".format(s))\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    if \"Be the first to review this employer.\" in driver.page_source:\n",
    "        print('No data available on Indeed')\n",
    "        df_main_indeed = pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    if \" reviews matching the search\" in driver.page_source:\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        if \"We can't find this page\" in driver.page_source:\n",
    "            print('No data found on Indeed')\n",
    "            df_main_indeed = pd.DataFrame\n",
    "        else:\n",
    "            #sign in the browser first\n",
    "            #using BeautifulSoup4 to scrape out page source of the webpage\n",
    "            while True:\n",
    "                # Search for \"Review this company\"\" on the webpage\n",
    "                if \"Review this company\" in driver.page_source:\n",
    "                    data = requests.get(url)\n",
    "                    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                    # print(soup)\n",
    "                    break  # Exit the loop if the text is found\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            def complete_reviews(soup):\n",
    "                #extracting all data from customer-reviews section\n",
    "\n",
    "                #review_text of review\n",
    "                reviews_text = []\n",
    "                temp6 = soup.find_all('span', attrs={\"css-1cxc9zk e1wnkr790\"})\n",
    "                for i in temp6:\n",
    "                    reviews_text.append(i.text)\n",
    "                reviews_text = reviews_text[1:]\n",
    "\n",
    "                star_ratings = []\n",
    "                star = soup.find_all('button', attrs={'class': 'css-1c33izo e1wnkr790'})\n",
    "                for i in star:\n",
    "                    star_ratings.append(i.text)\n",
    "\n",
    "                #position of each customer\n",
    "                pos_of_customer = []\n",
    "                for pos in soup.find_all('div', attrs={'class': 'css-lw17hn eu4oa1w0'}):\n",
    "                    pos1 = pos.find('a', attrs={'class': 'css-rt1o3i e19afand0'})\n",
    "                    if pos1:\n",
    "                        pos_of_customer.append(pos1.text)\n",
    "                    else:\n",
    "                        pos_of_customer.append('NA')\n",
    "\n",
    "                #complete line containing position, date etc. to extract out date and location\n",
    "                complete_line = []\n",
    "                for cl in soup.find_all('div', attrs={'class': 'css-lw17hn eu4oa1w0'}):\n",
    "                    cl1 = cl.find('span', attrs={'itemprop': 'author'})\n",
    "                    if cl1:\n",
    "                        complete_line.append(cl1.text)\n",
    "                    else:\n",
    "                        complete_line.append('NA')\n",
    "\n",
    "                #extracting date from complete_line list\n",
    "                #splitting by (-)\n",
    "                date_of_review = []\n",
    "                for i in complete_line:\n",
    "                    date_of_review.append(i.split('- ')[2])\n",
    "\n",
    "                #extracting employee status from complete_line list\n",
    "                #splitting by ('(') and then by (')')\n",
    "                employee_status = []\n",
    "                for item in complete_line:\n",
    "                    inner_list = []\n",
    "                    for subitem in item.split('('):\n",
    "                        inner_list.extend(subitem.split(')'))\n",
    "                    employee_status.append(inner_list[1])\n",
    "\n",
    "                \n",
    "                # Ensure all arrays have the same length\n",
    "                min_length = min(len(reviews_text), len(star_ratings), len(pos_of_customer), len(date_of_review), len(employee_status))\n",
    "                reviews_text = reviews_text[:min_length]\n",
    "                star_ratings = star_ratings[:min_length]\n",
    "                pos_of_customer = pos_of_customer[:min_length]\n",
    "                date_of_review = date_of_review[:min_length]\n",
    "                employee_status = employee_status[:min_length]\n",
    "\n",
    "                all_pages = {}\n",
    "                all_pages['Reviews'] = reviews_text\n",
    "                all_pages['Rating'] = star_ratings\n",
    "                all_pages['Designation'] = pos_of_customer\n",
    "                all_pages['Date'] = date_of_review\n",
    "                all_pages['Employee Status'] = employee_status\n",
    "                return all_pages\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            if \"Showing the only review\" in driver.page_source:\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                #extracting all data from customer-reviews section\n",
    "\n",
    "                #review_text of review\n",
    "                reviews_text = []\n",
    "                temp6 = soup.find_all('span', attrs={\"css-1cxc9zk e1wnkr790\"})\n",
    "                for i in temp6:\n",
    "                    reviews_text.append(i.text)\n",
    "                reviews_text = reviews_text[1:]\n",
    "\n",
    "                star_ratings = []\n",
    "                star = soup.find_all('button', attrs={'class': 'css-1c33izo e1wnkr790'})\n",
    "                for i in star:\n",
    "                    star_ratings.append(i.text)\n",
    "\n",
    "                #position of each customer\n",
    "                pos_of_customer = []\n",
    "                for pos in soup.find_all('div', attrs={'class': 'css-lw17hn eu4oa1w0'}):\n",
    "                    pos1 = pos.find('a', attrs={'class': 'css-rt1o3i e19afand0'})\n",
    "                    if pos1:\n",
    "                        pos_of_customer.append(pos1.text)\n",
    "                    else:\n",
    "                        pos_of_customer.append('NA')\n",
    "\n",
    "                #complete line containing position, date etc. to extract out date and location\n",
    "                complete_line = []\n",
    "                for cl in soup.find_all('div', attrs={'class': 'css-lw17hn eu4oa1w0'}):\n",
    "                    cl1 = cl.find('span', attrs={'itemprop': 'author'})\n",
    "                    if cl1:\n",
    "                        complete_line.append(cl1.text)\n",
    "                    else:\n",
    "                        complete_line.append('NA')\n",
    "\n",
    "                #extracting date from complete_line list\n",
    "                #splitting by (-)\n",
    "                date_of_review = []\n",
    "                for i in complete_line:\n",
    "                    date_of_review.append(i.split('- ')[2])\n",
    "\n",
    "                #extracting employee status from complete_line list\n",
    "                #splitting by ('(') and then by (')')\n",
    "                employee_status = []\n",
    "                for item in complete_line:\n",
    "                    inner_list = []\n",
    "                    for subitem in item.split('('):\n",
    "                        inner_list.extend(subitem.split(')'))\n",
    "                    employee_status.append(inner_list[1])\n",
    "\n",
    "                \n",
    "                # Ensure all arrays have the same length\n",
    "                min_length = min(len(reviews_text), len(star_ratings), len(pos_of_customer), len(date_of_review), len(employee_status))\n",
    "                reviews_text = reviews_text[:min_length]\n",
    "                star_ratings = star_ratings[:min_length]\n",
    "                pos_of_customer = pos_of_customer[:min_length]\n",
    "                date_of_review = date_of_review[:min_length]\n",
    "                employee_status = employee_status[:min_length]\n",
    "\n",
    "                all_pages = {}\n",
    "                all_pages['Reviews'] = reviews_text\n",
    "                all_pages['Rating'] = star_ratings\n",
    "                all_pages['Designation'] = pos_of_customer\n",
    "                all_pages['Date'] = date_of_review\n",
    "                all_pages['Employee Status'] = employee_status\n",
    "                df_main_indeed = pd.DataFrame(all_pages)\n",
    "                df_main_indeed[\"Source\"] = \"Indeed\"\n",
    "                print(\"\\nIndeed's Data Extraction | Completed\\n\")\n",
    "\n",
    "            else:\n",
    "                text1 = driver.find_element(By.XPATH, '//*[@id=\"cmp-container\"]/div/div[1]/main/div[2]/div[1]/div[1]/div[1]/span/b')\n",
    "                text1_content = text1.text.replace(',', '')\n",
    "                int_text1 = int(text1_content)\n",
    "                no_of_pages = int(int_text1/21)\n",
    "                end_val = (no_of_pages)*20\n",
    "                df_main_indeed = pd.DataFrame()\n",
    "                for i in tqdm(range(20, end_val+1, 20)):\n",
    "                    try:\n",
    "                        wait = WebDriverWait(driver, 100)\n",
    "                        try:\n",
    "                            wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, \".css-13qwiat.e71d0lh0[data-tn-element='next-page']\"))).click()\n",
    "                        except NoSuchElementException:\n",
    "                            # Element not found, break the loop\n",
    "                            break\n",
    "                        time.sleep(4)\n",
    "                        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                        all_reviews = complete_reviews(soup)\n",
    "                        df = pd.DataFrame(all_reviews)\n",
    "                        df_main_indeed = pd.concat([df_main_indeed, df], ignore_index=True)\n",
    "                        df_main_indeed = df_main_indeed.reset_index(drop=True)\n",
    "                    except:\n",
    "                        print(i)\n",
    "                    df_main_indeed[\"Source\"] = \"Indeed\"\n",
    "                print(\"\\nIndeed's Data Extraction | Completed\\n\")\n",
    "        \n",
    "def Twitter_scrape():\n",
    "    driver.get('https://twitter.com/')\n",
    "    main_page = driver.current_window_handle\n",
    "    time.sleep(5)\n",
    "\n",
    "    #Login procedure\n",
    "    driver.switch_to.frame(0)\n",
    "    login = driver.find_element('xpath','//*[@id=\"container\"]')\n",
    "    ActionChains(driver).move_to_element(login).click().perform()\n",
    "    login.click()\n",
    "    for handle in driver.window_handles:\n",
    "        if handle != main_page:\n",
    "            login_page = handle\n",
    "    driver.switch_to.window(login_page)\n",
    "    try:\n",
    "        log = driver.find_element('xpath', '//*[@id=\"identifierId\"]')\n",
    "        log.send_keys(email)\n",
    "        log.send_keys(Keys.RETURN)\n",
    "        driver.implicitly_wait(10)\n",
    "        pas =  driver.find_element('xpath', '//*[@id=\"password\"]/div[1]/div/div[1]/input')\n",
    "        pas.send_keys(p)\n",
    "        pas.send_keys(Keys.RETURN)\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        log = driver.find_element('xpath','//*[@role=\"link\"]')\n",
    "        log.click()\n",
    "    driver.switch_to.window(main_page)\n",
    "    time.sleep(5)\n",
    "\n",
    "    #Searching for the organisation\n",
    "    search = driver.find_element('xpath','//input[@placeholder=\"Search Twitter\"]')\n",
    "    search.click()\n",
    "    search.send_keys(f\"{s}\")\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)\n",
    "\n",
    "    #Scraper\n",
    "    def get_data(tweet):\n",
    "        try:\n",
    "            postdate = tweet.find_element('xpath','.//time').get_attribute('datetime')\n",
    "        except NoSuchElementException:\n",
    "            return\n",
    "        try:\n",
    "            post = tweet.find_element('xpath','.//div[@data-testid=\"tweetText\"]').text\n",
    "        except NoSuchElementException:\n",
    "            return\n",
    "        reply = tweet.find_element('xpath','//div[@data-testid=\"reply\"]').text\n",
    "        retweet = tweet.find_element('xpath','.//div[@data-testid=\"retweet\"]').text\n",
    "        like = tweet.find_element('xpath','.//div[@data-testid=\"like\"]').text\n",
    "        views = tweet.find_element('xpath','.//div[@role=\"group\"]/div[4]').text\n",
    "        \n",
    "        dets = (postdate, post, reply, retweet, like, views)\n",
    "        return dets\n",
    "\n",
    "    data = []\n",
    "    tweet_ids = set()\n",
    "    page_end = False\n",
    "    height = driver.execute_script(\"return window.pageYOffset;\")\n",
    "    while page_end !=True:\n",
    "        tweets = driver.find_elements('xpath','//article[@data-testid=\"tweet\"]')\n",
    "        for tweet in tweets:\n",
    "            dat = get_data(tweet)\n",
    "            if dat:\n",
    "                tweet_id = ''.join(dat)\n",
    "                if tweet_id not in tweet_ids:\n",
    "                    tweet_ids.add(tweet_id)\n",
    "                    data.append(dat)\n",
    "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if height == new_height:\n",
    "            page_end = True\n",
    "        else:\n",
    "            height = new_height\n",
    "    \n",
    "    T = pd.DataFrame(data,columns =['Posted on', 'Tweet', 'Reply', 'Retweets', 'Likes', 'Views'])\n",
    "    T = T.rename(columns = {'Tweet':'Reviews'})\n",
    "    T['Date']= pd.to_datetime(T['Posted on'], errors = 'coerce')\n",
    "    T['Date'] = T['Date'].dt.date\n",
    "    T = T[['Date','Reviews']]\n",
    "    T['Source'] = 'Twitter'\n",
    "    print(\"'\\nTwitter Data Extraction | Completed\\n'\")\n",
    "    return T\n",
    "\n",
    "def AmbitionBox_scrape():\n",
    "    driver.get(\"https://www.ambitionbox.com/\")\n",
    "    main_page = driver.current_window_handle\n",
    "    time.sleep(2)\n",
    "\n",
    "    #Login Procedure\n",
    "    login = driver.find_element('xpath','//*[@id=\"ambitionbox-header\"]/div/div[2]/div/div[4]/a')\n",
    "    ActionChains(driver).move_to_element(login).click().perform()\n",
    "    time.sleep(1)\n",
    "    g_log = driver.find_element('xpath','//*[@class=\"social-login__button google\"]')\n",
    "    ActionChains(driver).move_to_element(g_log).click().perform()\n",
    "    time.sleep(1)\n",
    "\n",
    "    for handle in driver.window_handles:\n",
    "        if handle != main_page:\n",
    "            login_page = handle\n",
    "    driver.switch_to.window(login_page)\n",
    "    try:\n",
    "        login = driver.find_element('xpath', '//*[@id=\"identifierId\"]')\n",
    "        login.send_keys(email)\n",
    "        login.send_keys(Keys.RETURN)\n",
    "        driver.implicitly_wait(10)\n",
    "        pas =  driver.find_element('xpath', '//*[@id=\"password\"]/div[1]/div/div[1]/input')\n",
    "        pas.send_keys(p)\n",
    "        pas.send_keys(Keys.RETURN)\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        if \"Use another account\" in driver.page_source:\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"view_container\"]/div/div/div[2]/div/div[1]/div/form/span/section/div/div/div/div/ul/li[1]/div/div[1]/div/div[2]/div[1]').click()\n",
    "            time.sleep(3)\n",
    "\n",
    "\n",
    "    driver.switch_to.window(main_page)\n",
    "    time.sleep(3)\n",
    "\n",
    "    #Searching for the organisation\n",
    "    search = driver.find_element('xpath','//*[@id=\"homeTypeahead\"]')\n",
    "    search.send_keys(s)\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)\n",
    "\n",
    "    current_url = driver.current_url\n",
    "    current_url = str(current_url)\n",
    "    filter_text = '?sort_by=latest'\n",
    "    filter_url = current_url + filter_text\n",
    "    driver.get(filter_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    def rev_scrape(driver):\n",
    "        soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "        like = []\n",
    "        dislike = []\n",
    "        rev_date = []\n",
    "        ind_rate = []\n",
    "        des = []\n",
    "        review = {}\n",
    "        rev = soup.find_all('p', attrs = {'class': 'body-medium overflow-wrap'})\n",
    "        r_date = soup.find_all('span', attrs = {'class': 'status caption-subdued'})\n",
    "        r = soup.find_all('span', attrs = {'class': 'avg-rating bold-Labels'})\n",
    "        d = soup.find_all('h2', attrs = {'class': 'bold-title-s review-title'})\n",
    "\n",
    "        for i in range(len(rev)):\n",
    "            if i % 2 == 0:\n",
    "                like.append(rev[i].text)\n",
    "            else:\n",
    "                dislike.append(rev[i].text)\n",
    "        for i in range(len(r_date)):\n",
    "            rev_date.append(r_date[i].text.replace('posted on', ''))\n",
    "            ind_rate.append(r[i].text)\n",
    "            des.append(d[i].text)\n",
    "        \n",
    "        review[\"Designation\"] = des\n",
    "        review[\"Rating\"] = ind_rate\n",
    "        review[\"Date of Review\"] = rev_date\n",
    "        review[\"Like\"] = like\n",
    "        review[\"Dislike\"] = dislike\n",
    "        return review\n",
    "    \n",
    "    i = True\n",
    "    AB=pd.DataFrame()\n",
    "    while(i):\n",
    "        try:\n",
    "            dict = rev_scrape(driver)\n",
    "            df = pd.DataFrame.from_dict(dict,orient='index').transpose()\n",
    "            AB = pd.concat([AB, df], ignore_index=True)\n",
    "            button_l = driver.find_element('xpath','//*[@class=\"icon-chevron-right next nav-btn\"]')\n",
    "            button_l.click()\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            i= False\n",
    "    \n",
    "    AB['Reviews'] = AB['Like'] + \"  \" + AB['Dislike']\n",
    "    AB = AB.rename(columns = {'Date of Review': 'Date'})\n",
    "    AB['Date'] = pd.to_datetime(AB['Date'])\n",
    "    AB['Date'] = AB['Date'].dt.date\n",
    "    AB = AB[['Date','Reviews','Rating']]\n",
    "    AB['Source'] = 'Ambition Box'\n",
    "    print(\"\\Ambition Box's Data Extraction | Completed\\n\")\n",
    "    return AB\n",
    "\n",
    "def Facebook_scrape():\n",
    "    driver.get(\"https://www.facebook.com\")\n",
    "    driver.find_element(By.XPATH, \"//*[@id='email']\").send_keys(email)              #entering email-id\n",
    "    driver.find_element(By.XPATH, \"//*[@id='email']\").send_keys(Keys.TAB)\n",
    "    driver.find_element(By.XPATH, \"//*[@id='pass']\").send_keys(p)               #entering password\n",
    "    driver.find_element(By.XPATH, \"//*[@id='pass']\").send_keys(Keys.RETURN)            #hitting enter\n",
    "    time.sleep(5)\n",
    "    # driver.find_element(By.CLASS_NAME, 'x1uvtmcs x4k7w5x x1h91t0o x1beo9mf xaigb6o x12ejxvf x3igimt xarpa2k xedcshv x1lytzrv x1t2pt76 x7ja8zs x1n2onr6 x1qrby5j x1jfb8zj').send_keys(Keys.ESCAPE)\n",
    "    #going to companies post webpage\n",
    "    if 'Find friends' in driver.page_source:\n",
    "        driver.get(f'https://www.facebook.com/search/posts?q={s}&filters=eyJyZWNlbnRfcG9zdHM6MCI6IntcIm5hbWVcIjpcInJlY2VudF9wb3N0c1wiLFwiYXJnc1wiOlwiXCJ9In0%3D')\n",
    "        time.sleep(3)\n",
    "\n",
    "    # Function to scroll to the bottom of the page (page 2023)\n",
    "    def scroll_to_bottom():\n",
    "        SCROLL_PAUSE_TIME = 2  # Adjust the pause time if needed  \n",
    "        while True:\n",
    "            # Scroll to the bottom of the page\n",
    "            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "            # Wait for the new content to load\n",
    "            WebDriverWait(driver, SCROLL_PAUSE_TIME).until(EC.invisibility_of_element_located((By.CLASS_NAME, \"uiMorePagerLoader\")))\n",
    "            if driver.find_elements(By.XPATH, \"//span[contains(text(), 'End of results')]\"):\n",
    "                break  # Reached the bottom of the page                                                                                                     \n",
    "    scroll_to_bottom()\n",
    "\n",
    "    #using BeautifulSoup4 to scrape out data from the posts\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    #scraping data from the posts from filter of latest reviews\n",
    "    #extracting text from the posts\n",
    "    text_list = []\n",
    "    for tl in soup.find_all('div', attrs={'class': 'x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z'}):\n",
    "        tl1 = tl.find('span',attrs={'class': 'x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h'})\n",
    "        if tl1:\n",
    "            text_list.append(tl1.text)\n",
    "    #making a dictionary containing all the list to make the final dataframe\n",
    "    complete_list = {\"Reviews\":text_list}\n",
    "\n",
    "    #creating final dataframe\n",
    "    df_main = pd.DataFrame(complete_list)\n",
    "    df_main[\"Source\"] = \"Facebook\"\n",
    "    print(\"\\nFacebook's Data Extraction | Completed\\n\")\n",
    "    return df_main\n",
    "\n",
    "def TOI_scrape():                                            #opening browser\n",
    "    driver.get('https://timesofindia.indiatimes.com/advancesearch.cms')         #going to the video\n",
    "    driver.maximize_window()\n",
    "    time.sleep(1)\n",
    "\n",
    "    input_button = driver.find_element(By.XPATH, '//*[@id=\"frm1\"]/div[3]/input')\n",
    "    ActionChains(driver).move_to_element(input_button).click().send_keys(s).perform()\n",
    "\n",
    "    #setting yearly filter\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"frm1\"]/div[7]/span[1]/input').click()\n",
    "\n",
    "    #clicking submit button\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"frm1\"]/div[9]/div').click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    if \" - did not match any documents.\" in driver.page_source:\n",
    "        print('No data found on TOI')\n",
    "        df_main_toi = pd.DataFrame\n",
    "    else:\n",
    "        no_of_articles = driver.find_element(By.XPATH, '//*[@id=\"netspidersosh\"]/div[1]/div[2]/div/div[2]/div[2]/div[2]/span[3]')\n",
    "        no_of_articles = int(no_of_articles.text)\n",
    "\n",
    "        if no_of_articles<=10:\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            \n",
    "            #extracting reviews\n",
    "            reviews = []\n",
    "            for tl in soup.find_all('div', attrs={'style': 'padding-bottom:20px;font-size:13px;'}):\n",
    "                tl1 = tl.find('span', attrs={'style': 'font-family:arial'})\n",
    "                if tl1:\n",
    "                    reviews.append(tl1.text)\n",
    "            reviews = list(set(reviews))  \n",
    "\n",
    "            all_pages = {}\n",
    "            all_pages['Reviews'] = reviews\n",
    "            df_main_toi = pd.DataFrame(all_pages)\n",
    "            df_main_toi[\"Source\"] = \"Times of India\"\n",
    "            df_main_toi.dropna(subset=[\"Reviews\"], inplace=True)\n",
    "            print(\"\\nTOI's Data Extraction | Completed\\n\")\n",
    "            return df_main_toi\n",
    "\n",
    "        if no_of_articles>10:\n",
    "            no_of_scrolls = int(no_of_articles/10)\n",
    "            def complete_reviews(soup):\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                #extracting reviews\n",
    "                reviews = []\n",
    "                for tl in soup.find_all('div', attrs={'style': 'padding-bottom:20px;font-size:13px;'}):\n",
    "                    tl1 = tl.find('span', attrs={'style': 'font-family:arial'})\n",
    "                    if tl1:\n",
    "                        reviews.append(tl1.text)\n",
    "                reviews = list(set(reviews))  \n",
    "\n",
    "                all_pages = {}\n",
    "                all_pages['Reviews'] = reviews\n",
    "                return all_pages\n",
    "\n",
    "            df_main_toi=pd.DataFrame()\n",
    "            for i in tqdm(range(1,no_of_scrolls+1,1)):\n",
    "                try:\n",
    "                    url = f\"https://timesofindia.indiatimes.com/searchresult.cms?maxRow=&kdaterange=365&article=2&searchtype=2&section=&searchField=&query={s}&pagenumber={i}&startdate=2001-01-01&enddate=2001-07-26&date1mm=01&date1dd=01&date1yyyy=2001&date2mm=07&date2dd=26&date2yyyy=2001\"\n",
    "                    driver.get(url)\n",
    "                    soup  = BeautifulSoup(driver.page_source,'lxml')\n",
    "                    dict = complete_reviews(soup)\n",
    "                    df=pd.DataFrame.from_dict(dict,orient='index').transpose()\n",
    "                    df_main_toi = pd.concat([df_main_toi, df], ignore_index=True)\n",
    "                except:\n",
    "                    print(i)\n",
    "            df_main_toi[\"Source\"] = \"Times of India\"\n",
    "            df_main_toi.dropna(subset=[\"Reviews\"], inplace=True)\n",
    "            print(\"\\nTOI's Data Extraction | Completed\\n\")\n",
    "            return df_main_toi\n",
    "    \n",
    "def LinkedIn_scrape():\n",
    "    driver.get('https://www.linkedin.com')\n",
    "    actions.login(driver, email2, p2)\n",
    "\n",
    "    # Searching for the organisation\n",
    "    search = driver.find_element('xpath','//*[@id=\"global-nav-typeahead\"]/input')\n",
    "    search.send_keys(s)\n",
    "    search.send_keys(Keys.RETURN)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Check for any known company\n",
    "    fill = driver.find_elements('xpath','//*[@class = \"search-reusables__primary-filter\"]')\n",
    "    for i in fill:\n",
    "        if i.text == 'Companies':\n",
    "            i.click()\n",
    "            break\n",
    "    else:\n",
    "        print(\"No such organisation is on LinkedIn.\")\n",
    "    try:\n",
    "        if \"No results for\" in driver.find_element('xpath','//*[@class = \"t-14 t-black--light\"]').text:\n",
    "                print(\"No results found for the searched entity\")\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(5)\n",
    "    # First search result\n",
    "    comp = driver.find_elements('xpath','//*[@class=\"reusable-search__result-container\"]')\n",
    "    for i in range(len(comp)):\n",
    "         if 'Follow' in comp[i].text:\n",
    "                comp[i].click()\n",
    "                break\n",
    "         else:\n",
    "                pass\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Post's Tab\n",
    "    try:\n",
    "        for i in range(1,6):\n",
    "            tab = driver.find_element('xpath',f'//*[@aria-label=\"Organization’s page navigation\"]/ul/li[{i}]')\n",
    "            if tab.text == \"Posts\":\n",
    "                tab.click()\n",
    "    except:\n",
    "        print(\"No Post's Tab found!\")\n",
    "\n",
    "    def scrolling(driver):\n",
    "        page_end = False\n",
    "        height = 0\n",
    "        while page_end !=True:\n",
    "            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            time.sleep(3)\n",
    "            if height == new_height:\n",
    "                page_end = True\n",
    "            else:\n",
    "                height = new_height\n",
    "    \n",
    "    scrolling(driver)\n",
    "    posts = []\n",
    "    dets = []\n",
    "    soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "    post = driver.find_elements('xpath', '//div[@class=\"ember-view  occludable-update \"]')\n",
    "    det = soup.find_all('ul', attrs = {'class': 'social-details-social-counts'})\n",
    "\n",
    "    for i in range(len(post)):\n",
    "        posts.append(post[i].text.replace('\\n', ''))\n",
    "        dets.append(det[i].text.replace('\\n', ''))\n",
    "\n",
    "    Post = []\n",
    "    import re\n",
    "    for i in range(len(posts)):\n",
    "        temp = re.split('•Follow|…see more',posts[i])\n",
    "        Post.append(temp[1])\n",
    "    like = []\n",
    "    comment = [None] * len(dets)\n",
    "    repost = [None] * len(dets)\n",
    "    for i in range(len(dets)):\n",
    "        temp = dets[i].split(\"  \")\n",
    "        like.append(temp[0])\n",
    "        for j in range(len(temp)):\n",
    "            if 'comments' in temp[j]:\n",
    "                comment[i] = temp[j]\n",
    "            if 'reposts' in temp[j]:\n",
    "                repost[i] = temp[j]\n",
    "    \n",
    "    L = pd.DataFrame(list(zip(Post, like,comment,repost)), columns =['Post', 'Likes','Comments','Reposts'])\n",
    "    L['Date'] = None\n",
    "    L = L.rename(columns = {'Post':'Reviews'})\n",
    "    L = L[['Date','Reviews']]\n",
    "    L['Source'] = 'LinkedIn'\n",
    "    print(\"\\nLinkedIn's Data Extraction | Completed\\n\")\n",
    "    return L\n",
    "\n",
    "def Glassdoor_scrape():\n",
    "    #using selenium for automation\n",
    "    url = \"https://www.glassdoor.co.in/member/home/index.htm\"\n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    driver.back()\n",
    "    driver.forward()\n",
    "    global s\n",
    "    #logging-in\n",
    "    wait = WebDriverWait(driver, 1000)\n",
    "    if wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"InlineLoginModule\"]/div/div/div[1]/div/div/div/div/form/div[1]/label'))):\n",
    "        #entering email\n",
    "        enter_email = driver.find_element(By.XPATH, '//*[@id=\"inlineUserEmail\"]')\n",
    "        ActionChains(driver).move_to_element(enter_email).click().send_keys(email).send_keys(Keys.RETURN).perform()\n",
    "    else:\n",
    "        time.sleep(2)\n",
    "        #entering password\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    if wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"InlineLoginModule\"]/div/div/div[1]/div/div/div/div/button'))):\n",
    "        enter_password = driver.find_element(By.XPATH, '//*[@id=\"inlineUserPassword\"]')\n",
    "        ActionChains(driver).move_to_element(enter_password).click().send_keys(p).send_keys(Keys.RETURN).perform()\n",
    "\n",
    "    time.sleep(2)\n",
    "    driver.get(url)                 #sometimes, some alternate page opens up, in that case, this line is required\n",
    "\n",
    "    if searches == 'rigi' or searches == 'Rigi' or searches == 'RIGI':\n",
    "        s = 'Rigi (India)'\n",
    "\n",
    "    #waiting for page to load\n",
    "    time.sleep(3)\n",
    "    driver.get('https://www.glassdoor.co.in/Reviews/index.htm?overall_rating_low=3.5&page=1&locId=2921225&locType=C&occ=Data%20Analyst')\n",
    "\n",
    "    #entering comapany's name in search box \n",
    "    wait = WebDriverWait(driver, 100)\n",
    "    if wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Explore\"]/div[2]/div/div/div[1]'))):\n",
    "        enter_company = driver.find_element(By.XPATH, '//*[@id=\"companyAutocomplete-companyDiscover-employerSearch\"]')\n",
    "        ActionChains(driver).move_to_element(enter_company).click().send_keys(s).perform()\n",
    "    else:\n",
    "        time.sleep(2)\n",
    "\n",
    "    driver.find_element(By.XPATH, '//*[@id=\"Explore\"]/div[2]/div/div/div[2]/button').click()\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    if \" Showing results for \" in driver.page_source:\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        if \"Check your spelling\" in driver.page_source:\n",
    "            print('No data found on Glassdoor')\n",
    "            global df_main_glassdoor\n",
    "            df_main_glassdoor = pd.DataFrame\n",
    "        else:\n",
    "            time.sleep(5)\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"MainCol\"]/div/div[1]/div/div[1]/div/div[2]/h2/a').click()\n",
    "\n",
    "            time.sleep(5) \n",
    "            driver.find_element(By.XPATH, '//*[@id=\"EmpLinksWrapper\"]/div[2]/div/div[1]/a[1]').click()\n",
    "\n",
    "            #chosing filter\n",
    "            time.sleep(3)\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[1]/div/div[1]/div[2]/div[2]/button').click()\n",
    "            time.sleep(2)\n",
    "            loc_filter = driver.find_element(By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[1]/div/div[1]/div[3]/div[2]/div[1]/div/div[1]')\n",
    "            ActionChains(driver).move_to_element(loc_filter).click().click().send_keys('India ').send_keys(Keys.RETURN).perform()\n",
    "\n",
    "            #waiting for filtered-review page to load and then finding soup\n",
    "            wait = WebDriverWait(driver, 100)\n",
    "            if wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[3]/div[1]/div/span'))):\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            else:\n",
    "                time.sleep(2)\n",
    "            #extracting all data from customer-reviews section in a function \n",
    "\n",
    "\n",
    "            def complete_reviews(soup):\n",
    "\n",
    "                #extracting the pro reviews from all the reviews\n",
    "                pros_reviews = []\n",
    "                for pros in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    pros1 = pros.find('span', attrs={'data-test': \"pros\"})\n",
    "                    if pros1:\n",
    "                        pros_reviews.append(pros1.text)\n",
    "                    else:\n",
    "                        pros_reviews.append('NA')\n",
    "\n",
    "                #extracting the cons reviews from all the reviews\n",
    "                cons_reviews = []\n",
    "                for cons in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    cons1 = cons.find('span', attrs={'data-test': \"cons\"})\n",
    "                    if cons1:\n",
    "                        cons_reviews.append(cons1.text)\n",
    "                    else:\n",
    "                        cons_reviews.append('NA')\n",
    "\n",
    "                #saving the pros and cons in one column named \"Reviews\"\n",
    "                combined_reviews = list(zip(pros_reviews, cons_reviews))\n",
    "\n",
    "                #extracting the star ratings from the reviews\n",
    "                star_ratings = []\n",
    "                for star in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    star1 = star.find('span', attrs={'class': 'ratingNumber mr-xsm'})\n",
    "                    if star1:\n",
    "                        star_ratings.append(star1.text)\n",
    "                    else:\n",
    "                        star_ratings.append('NA')\n",
    "\n",
    "                #extracting the employee status line to extract employee status from it\n",
    "                emp_line = []\n",
    "                for empl in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    empl1 = empl.find('span', attrs={'class': 'pt-xsm pt-md-0 css-1qxtz39 eg4psks0'})\n",
    "                    if empl1:\n",
    "                        emp_line.append(empl1.text)\n",
    "                    else:\n",
    "                        emp_line.append('NA')\n",
    "                \n",
    "                # extracting status of employee\n",
    "                employee_status = []\n",
    "                for i in emp_line:\n",
    "                    employee_status.append(i.split(',')[0])\n",
    "                \n",
    "                #extracting the line that contains the date and designation in the review\n",
    "                date_and_des = []\n",
    "                for ded in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    ded1 = ded.find('span', attrs={'class': 'middle common__EiReviewDetailsStyle__newGrey'})\n",
    "                    if ded1:\n",
    "                        date_and_des.append(ded1.text)\n",
    "                    else:\n",
    "                        date_and_des.append('NA')\n",
    "\n",
    "                #extracting the date from the date_and_des list\n",
    "                date_of_review = []\n",
    "                for i in date_and_des:\n",
    "                    date_of_review.append(i.split(' -')[0])\n",
    "\n",
    "                #extracting the designation from the date_and_des list\n",
    "                designation_of_reviewer = []\n",
    "                for i in date_and_des:\n",
    "                    designation_of_reviewer.append(i.split('- ')[-1])\n",
    "\n",
    "                all_pages = {}\n",
    "                all_pages['Reviews'] = combined_reviews\n",
    "                all_pages['Rating'] = star_ratings\n",
    "                all_pages['Designation'] = designation_of_reviewer\n",
    "                all_pages['Date'] = date_of_review\n",
    "                all_pages['Employee Status'] = employee_status\n",
    "\n",
    "                return all_pages\n",
    "\n",
    "            #counting the number of pages to be paginated\n",
    "            total_reviews = driver.find_element(By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[3]/div[1]/h2/span/strong[1]')\n",
    "            total_reviews = int(total_reviews.text.replace(',',''))       #total number of reviews after filter\n",
    "            no_of_pages = int(total_reviews/10)                                #number of pages to be scrolled\n",
    "            \n",
    "            if no_of_pages<1:\n",
    "                soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                #extracting all data from customer-reviews section in a function \n",
    "\n",
    "                #extracting the pro reviews from all the reviews\n",
    "                pros_reviews = []\n",
    "                for pros in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    pros1 = pros.find('span', attrs={'data-test': \"pros\"})\n",
    "                    if pros1:\n",
    "                        pros_reviews.append(pros1.text)\n",
    "                    else:\n",
    "                        pros_reviews.append('NA')\n",
    "\n",
    "                #extracting the cons reviews from all the reviews\n",
    "                cons_reviews = []\n",
    "                for cons in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    cons1 = cons.find('span', attrs={'data-test': \"cons\"})\n",
    "                    if cons1:\n",
    "                        cons_reviews.append(cons1.text)\n",
    "                    else:\n",
    "                        cons_reviews.append('NA')\n",
    "\n",
    "                #saving the pros and cons in one column named \"Reviews\"\n",
    "                combined_reviews = list(zip(pros_reviews, cons_reviews))\n",
    "\n",
    "                #extracting the star ratings from the reviews\n",
    "                star_ratings = []\n",
    "                for star in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    star1 = star.find('span', attrs={'class': 'ratingNumber mr-xsm'})\n",
    "                    if star1:\n",
    "                        star_ratings.append(star1.text)\n",
    "                    else:\n",
    "                        star_ratings.append('NA')\n",
    "\n",
    "                #extracting the employee status line to extract employee status from it\n",
    "                emp_line = []\n",
    "                for empl in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    empl1 = empl.find('span', attrs={'class': 'pt-xsm pt-md-0 css-1qxtz39 eg4psks0'})\n",
    "                    if empl1:\n",
    "                        emp_line.append(empl1.text)\n",
    "                    else:\n",
    "                        emp_line.append('NA')\n",
    "\n",
    "                # extracting status of employee\n",
    "                employee_status = []\n",
    "                for i in emp_line:\n",
    "                    employee_status.append(i.split(',')[0])\n",
    "                \n",
    "                #extracting the line that contains the date and designation in the review\n",
    "                date_and_des = []\n",
    "                for ded in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                    ded1 = ded.find('span', attrs={'class': 'middle common__EiReviewDetailsStyle__newGrey'})\n",
    "                    if ded1:\n",
    "                        date_and_des.append(ded1.text)\n",
    "                    else:\n",
    "                        date_and_des.append('NA')\n",
    "\n",
    "                #extracting the date from the date_and_des list\n",
    "                date_of_review = []\n",
    "                for i in date_and_des:\n",
    "                    date_of_review.append(i.split(' -')[0])\n",
    "\n",
    "                #extracting the designation from the date_and_des list\n",
    "                designation_of_reviewer = []\n",
    "                for i in date_and_des:\n",
    "                    designation_of_reviewer.append(i.split('- ')[-1])\n",
    "\n",
    "                all_pages = {}\n",
    "                all_pages['Reviews'] = combined_reviews\n",
    "                all_pages['Rating'] = star_ratings\n",
    "                all_pages['Designation'] = designation_of_reviewer\n",
    "                all_pages['Date'] = date_of_review\n",
    "                all_pages['Employee Status'] = employee_status\n",
    "\n",
    "                df_main_glassdoor = pd.DataFrame(all_pages)\n",
    "                df_main_glassdoor[\"Source\"] = \"Glassdoor\"\n",
    "                print(\"\\nGlassdoor's Data Extraction | Completed\\n\")\n",
    "            \n",
    "            if no_of_pages >= 1:\n",
    "                # Creating final dataframe & pagination using link\n",
    "                df_main_glassdoor = pd.DataFrame()\n",
    "                wait = WebDriverWait(driver, 100)\n",
    "                try:\n",
    "                    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[4]/div/div[1]/button[2]'))).click()\n",
    "                except NoSuchElementException:\n",
    "                    # If the first XPath is not found, try the second one\n",
    "                    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[6]/div/div[1]/button[2]'))).click()\n",
    "                for i in tqdm(range(1, no_of_pages + 1, 1)):\n",
    "                    wait = WebDriverWait(driver, 100)\n",
    "                    try:\n",
    "                        # try:\n",
    "                        #     wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[6]/div/div[1]/button[2]'))).click()\n",
    "                        # except NoSuchElementException:\n",
    "                        #     # If the first XPath is not found, try the second one\n",
    "                        wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[6]/div/div[1]/button[2]'))).click()\n",
    "                    \n",
    "                        time.sleep(4)\n",
    "                        # Scroll to the next button element\n",
    "                        next_button = driver.find_element(By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[6]/div/div[1]/button[2]')\n",
    "                        actions = ActionChains(driver)\n",
    "                        actions.move_to_element(next_button).perform()\n",
    "                        \n",
    "                        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                        all_reviews = complete_reviews(soup)\n",
    "                        df = pd.DataFrame(all_reviews)\n",
    "                        df_main_glassdoor = pd.concat([df_main_glassdoor, df], ignore_index=True)\n",
    "                        df_main_glassdoor = df_main_glassdoor.reset_index(drop=True)\n",
    "                    except:\n",
    "                        print(i)\n",
    "\n",
    "\n",
    "\n",
    "                df_main_glassdoor[\"Source\"] = \"Glassdoor\"\n",
    "                print(\"\\nGlassdoor's Data Extraction | Completed\\n\")\n",
    "    else:\n",
    "        time.sleep(4) \n",
    "        driver.find_element(By.XPATH, '//*[@id=\"EmpLinksWrapper\"]/div[2]/div/div[1]/a[1]').click()\n",
    "\n",
    "        #chosing filter\n",
    "        wait = WebDriverWait(driver, 100)\n",
    "        if wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[3]/div[1]/h2'))):\n",
    "            driver.find_element(By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[1]/div/div[1]/div[2]/div[2]/button').click()\n",
    "            time.sleep(2)\n",
    "            loc_filter = driver.find_element(By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[1]/div/div[1]/div[3]/div[2]/div[1]/div/div[1]')\n",
    "            ActionChains(driver).move_to_element(loc_filter).click().click().send_keys('India ').send_keys(Keys.RETURN).perform()\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "\n",
    "        #waiting for filtered-review page to load and then finding soup\n",
    "        wait = WebDriverWait(driver, 100)\n",
    "        if wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[3]/div[1]/div/span'))):\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "        #extracting all data from customer-reviews section in a function \n",
    "\n",
    "\n",
    "        def complete_reviews(soup):\n",
    "\n",
    "            #extracting the pro reviews from all the reviews\n",
    "            pros_reviews = []\n",
    "            for pros in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                pros1 = pros.find('span', attrs={'data-test': \"pros\"})\n",
    "                if pros1:\n",
    "                    pros_reviews.append(pros1.text)\n",
    "                else:\n",
    "                    pros_reviews.append('NA')\n",
    "\n",
    "            #extracting the cons reviews from all the reviews\n",
    "            cons_reviews = []\n",
    "            for cons in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                cons1 = cons.find('span', attrs={'data-test': \"cons\"})\n",
    "                if cons1:\n",
    "                    cons_reviews.append(cons1.text)\n",
    "                else:\n",
    "                    cons_reviews.append('NA')\n",
    "\n",
    "            #saving the pros and cons in one column named \"Reviews\"\n",
    "            combined_reviews = list(zip(pros_reviews, cons_reviews))\n",
    "\n",
    "            #extracting the star ratings from the reviews\n",
    "            star_ratings = []\n",
    "            for star in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                star1 = star.find('span', attrs={'class': 'ratingNumber mr-xsm'})\n",
    "                if star1:\n",
    "                    star_ratings.append(star1.text)\n",
    "                else:\n",
    "                    star_ratings.append('NA')\n",
    "\n",
    "            #extracting the employee status line to extract employee status from it\n",
    "            emp_line = []\n",
    "            for empl in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                empl1 = empl.find('span', attrs={'class': 'pt-xsm pt-md-0 css-1qxtz39 eg4psks0'})\n",
    "                if empl1:\n",
    "                    emp_line.append(empl1.text)\n",
    "                else:\n",
    "                    emp_line.append('NA')\n",
    "            \n",
    "            # extracting status of employee\n",
    "            employee_status = []\n",
    "            for i in emp_line:\n",
    "                employee_status.append(i.split(',')[0])\n",
    "            \n",
    "            #extracting the line that contains the date and designation in the review\n",
    "            date_and_des = []\n",
    "            for ded in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                ded1 = ded.find('span', attrs={'class': 'middle common__EiReviewDetailsStyle__newGrey'})\n",
    "                if ded1:\n",
    "                    date_and_des.append(ded1.text)\n",
    "                else:\n",
    "                    date_and_des.append('NA')\n",
    "\n",
    "            #extracting the date from the date_and_des list\n",
    "            date_of_review = []\n",
    "            for i in date_and_des:\n",
    "                date_of_review.append(i.split(' -')[0])\n",
    "\n",
    "            #extracting the designation from the date_and_des list\n",
    "            designation_of_reviewer = []\n",
    "            for i in date_and_des:\n",
    "                designation_of_reviewer.append(i.split('- ')[-1])\n",
    "\n",
    "            all_pages = {}\n",
    "            all_pages['Reviews'] = combined_reviews\n",
    "            all_pages['Rating'] = star_ratings\n",
    "            all_pages['Designation'] = designation_of_reviewer\n",
    "            all_pages['Date'] = date_of_review\n",
    "            all_pages['Employee Status'] = employee_status\n",
    "\n",
    "            return all_pages\n",
    "\n",
    "        #counting the number of pages to be paginated\n",
    "        total_reviews = driver.find_element(By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[1]/div[3]/div[1]/h2/span/strong[1]')\n",
    "        total_reviews = int(total_reviews.text.replace(',',''))       #total number of reviews after filter\n",
    "        no_of_pages = int(total_reviews/10)                                #number of pages to be scrolled\n",
    "        \n",
    "        if no_of_pages<1:\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            #extracting all data from customer-reviews section in a function \n",
    "\n",
    "            #extracting the pro reviews from all the reviews\n",
    "            pros_reviews = []\n",
    "            for pros in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                pros1 = pros.find('span', attrs={'data-test': \"pros\"})\n",
    "                if pros1:\n",
    "                    pros_reviews.append(pros1.text)\n",
    "                else:\n",
    "                    pros_reviews.append('NA')\n",
    "\n",
    "            #extracting the cons reviews from all the reviews\n",
    "            cons_reviews = []\n",
    "            for cons in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                cons1 = cons.find('span', attrs={'data-test': \"cons\"})\n",
    "                if cons1:\n",
    "                    cons_reviews.append(cons1.text)\n",
    "                else:\n",
    "                    cons_reviews.append('NA')\n",
    "\n",
    "            #saving the pros and cons in one column named \"Reviews\"\n",
    "            combined_reviews = list(zip(pros_reviews, cons_reviews))\n",
    "\n",
    "            #extracting the star ratings from the reviews\n",
    "            star_ratings = []\n",
    "            for star in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                star1 = star.find('span', attrs={'class': 'ratingNumber mr-xsm'})\n",
    "                if star1:\n",
    "                    star_ratings.append(star1.text)\n",
    "                else:\n",
    "                    star_ratings.append('NA')\n",
    "\n",
    "            #extracting the employee status line to extract employee status from it\n",
    "            emp_line = []\n",
    "            for empl in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                empl1 = empl.find('span', attrs={'class': 'pt-xsm pt-md-0 css-1qxtz39 eg4psks0'})\n",
    "                if empl1:\n",
    "                    emp_line.append(empl1.text)\n",
    "                else:\n",
    "                    emp_line.append('NA')\n",
    "\n",
    "            # extracting status of employee\n",
    "            employee_status = []\n",
    "            for i in emp_line:\n",
    "                employee_status.append(i.split(',')[0])\n",
    "            \n",
    "            #extracting the line that contains the date and designation in the review\n",
    "            date_and_des = []\n",
    "            for ded in soup.find_all('div', attrs={'class': 'gdReview'}):\n",
    "                ded1 = ded.find('span', attrs={'class': 'middle common__EiReviewDetailsStyle__newGrey'})\n",
    "                if ded1:\n",
    "                    date_and_des.append(ded1.text)\n",
    "                else:\n",
    "                    date_and_des.append('NA')\n",
    "\n",
    "            #extracting the date from the date_and_des list\n",
    "            date_of_review = []\n",
    "            for i in date_and_des:\n",
    "                date_of_review.append(i.split(' -')[0])\n",
    "\n",
    "            #extracting the designation from the date_and_des list\n",
    "            designation_of_reviewer = []\n",
    "            for i in date_and_des:\n",
    "                designation_of_reviewer.append(i.split('- ')[-1])\n",
    "\n",
    "            all_pages = {}\n",
    "            all_pages['Reviews'] = combined_reviews\n",
    "            all_pages['Rating'] = star_ratings\n",
    "            all_pages['Designation'] = designation_of_reviewer\n",
    "            all_pages['Date'] = date_of_review\n",
    "            all_pages['Employee Status'] = employee_status\n",
    "\n",
    "            df_main_glassdoor = pd.DataFrame(all_pages)\n",
    "            df_main_glassdoor[\"Source\"] = \"Glassdoor\"\n",
    "            print(\"\\nGlassdoor's Data Extraction | Completed\\n\")\n",
    "            return df_main_glassdoor\n",
    "        \n",
    "        if no_of_pages >= 1:\n",
    "            # Creating final dataframe & pagination using link\n",
    "            df_main_glassdoor = pd.DataFrame()\n",
    "            wait = WebDriverWait(driver, 100)\n",
    "            try:\n",
    "                wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[4]/div/div[1]/button[2]'))).click()\n",
    "            except NoSuchElementException:\n",
    "                # If the first XPath is not found, try the second one\n",
    "                wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[6]/div/div[1]/button[2]'))).click()\n",
    "            for i in tqdm(range(1, no_of_pages + 1, 1)):\n",
    "                wait = WebDriverWait(driver, 100)\n",
    "                try:\n",
    "                    # try:\n",
    "                    #     wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[6]/div/div[1]/button[2]'))).click()\n",
    "                    # except NoSuchElementException:\n",
    "                    #     # If the first XPath is not found, try the second one\n",
    "                    wait.until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[6]/div/div[1]/button[2]'))).click()\n",
    "                \n",
    "                    time.sleep(4)\n",
    "                    # Scroll to the next button element\n",
    "                    next_button = driver.find_element(By.XPATH, '//*[@id=\"Container\"]/div[1]/div[2]/main/div[6]/div/div[1]/button[2]')\n",
    "                    actions = ActionChains(driver)\n",
    "                    actions.move_to_element(next_button).perform()\n",
    "                    \n",
    "                    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                    all_reviews = complete_reviews(soup)\n",
    "                    df = pd.DataFrame(all_reviews)\n",
    "                    df_main_glassdoor = pd.concat([df_main_glassdoor, df], ignore_index=True)\n",
    "                    df_main_glassdoor = df_main_glassdoor.reset_index(drop=True)\n",
    "                except:\n",
    "                    print(i)\n",
    "\n",
    "\n",
    "\n",
    "            df_main_glassdoor[\"Source\"] = \"Glassdoor\"\n",
    "            print(\"\\nGlassdoor's Data Extraction | Completed\\n\")\n",
    "            return df_main_glassdoor\n",
    "\n",
    "def Entrackr_scrape():\n",
    "    driver.get(f'https://entrackr.com/?s={s}')\n",
    "    driver.maximize_window()\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    if soup.find_all('div', attrs={'class': 'elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-99d9251'}):\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        if \"You searched for \" in driver.page_source:\n",
    "\n",
    "            def complete_reviews(soup):\n",
    "                text_line = []\n",
    "                for text in soup.find_all('div', attrs={'class': 'elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-99d9251'}):\n",
    "                    text1 = text.find('div', attrs={'class': 'elementor-element elementor-element-0a9137c elementor-widget elementor-widget-theme-post-title elementor-page-title elementor-widget-heading'})\n",
    "                    if text1:\n",
    "                        text_line.append(text1.text)\n",
    "                    else:\n",
    "                        text_line.append('NA')\n",
    "\n",
    "                news_text = []\n",
    "                for i in text_line:\n",
    "                    news_text.append(i[2:-2])\n",
    "                \n",
    "                date_line = []\n",
    "                for date in soup.find_all('div', attrs={'class': 'elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-99d9251'}):\n",
    "                    date1 = date.find('span', attrs={'class': 'elementor-icon-list-text elementor-post-info__item elementor-post-info__item--type-date'})\n",
    "                    if date1:\n",
    "                        date_line.append(date1.text)\n",
    "                    else:\n",
    "                        date_line.append('NA')\n",
    "\n",
    "                news_date = []\n",
    "                for i in date_line:\n",
    "                    news_date.append(i[11:-5])\n",
    "\n",
    "                all_pages = {}\n",
    "                all_pages['Reviews'] = news_text\n",
    "                all_pages['Date'] = news_date\n",
    "                return all_pages\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        page1_news_count = soup.find_all('div', attrs={'class':'elementor-element elementor-element-0a9137c elementor-widget elementor-widget-theme-post-title elementor-page-title elementor-widget-heading'})\n",
    "        \n",
    "\n",
    "        if len(page1_news_count)<=24:\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            text_line = []\n",
    "            for text in soup.find_all('div', attrs={'class': 'elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-99d9251'}):\n",
    "                text1 = text.find('div', attrs={'class': 'elementor-element elementor-element-0a9137c elementor-widget elementor-widget-theme-post-title elementor-page-title elementor-widget-heading'})\n",
    "                if text1:\n",
    "                    text_line.append(text1.text)\n",
    "                else:\n",
    "                    text_line.append('NA')\n",
    "                    \n",
    "            news_text = []\n",
    "            for i in text_line:\n",
    "                news_text.append(i[2:-2])\n",
    "            \n",
    "            date_line = []\n",
    "            for date in soup.find_all('div', attrs={'class': 'elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-99d9251'}):\n",
    "                date1 = date.find('span', attrs={'class': 'elementor-icon-list-text elementor-post-info__item elementor-post-info__item--type-date'})\n",
    "                if date1:\n",
    "                    date_line.append(date1.text)\n",
    "                else:\n",
    "                    date_line.append('NA')\n",
    "\n",
    "            news_date = []\n",
    "            for i in date_line:\n",
    "                news_date.append(i[11:-5])\n",
    "\n",
    "            all_pages = {}\n",
    "            all_pages['Reviews'] = news_text\n",
    "            all_pages['Date'] = news_date\n",
    "            global df_main_entrackr\n",
    "            df_main_entrackr = pd.DataFrame(all_pages)\n",
    "            df_main_entrackr['Source'] = 'Entrackr'\n",
    "            print('\\nEntrackr Data Extraction | Completed\\n')\n",
    "        \n",
    "        elif \"Next »\" in driver.page_source:\n",
    "                #counting the number of pages to scroll\n",
    "                total_pages = driver.find_element(By.XPATH, '/html/body/div[2]/div/section/div/div/div[1]/div/div/div[2]/div/nav/a[3]').text\n",
    "                total_pages = int(total_pages)\n",
    "                #creating final dataframe & pagination using link\n",
    "                df_main_entrackr = pd.DataFrame()\n",
    "                try:\n",
    "                    # Extract the reviews from the first page before proceeding to pagination\n",
    "                    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                    all_reviews = complete_reviews(soup)\n",
    "                    df = pd.DataFrame(all_reviews)\n",
    "                    df_main_entrackr = pd.concat([df_main_entrackr, df], ignore_index=True)\n",
    "                    df_main_entrackr = df_main_entrackr.reset_index(drop=True)\n",
    "\n",
    "                    for i in tqdm(range(2, total_pages+1)):  # Start from page 2 since page 1 data is already extracted\n",
    "                        wait = WebDriverWait(driver, 100)\n",
    "                        try:\n",
    "                            # Find the element with the text \"Next »\" and click it\n",
    "                            next_button = driver.find_element(By.CLASS_NAME, \"page-numbers.next\")\n",
    "                            next_button.click()\n",
    "\n",
    "                        except NoSuchElementException:\n",
    "                            print('Next button not found')\n",
    "                            break  # Exit the loop if Next button is not found\n",
    "\n",
    "                        # Wait for the page to load\n",
    "                        time.sleep(2)\n",
    "\n",
    "                        # Extract the reviews from the current page\n",
    "                        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "                        all_reviews = complete_reviews(soup)\n",
    "                        df = pd.DataFrame(all_reviews)\n",
    "                        df_main_entrackr = pd.concat([df_main_entrackr, df], ignore_index=True)\n",
    "                        df_main_entrackr = df_main_entrackr.reset_index(drop=True)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                df_main_entrackr['Source'] = 'Entrackr'\n",
    "                print('\\nEntrackr Data Extraction | Completed\\n')\n",
    "    else:\n",
    "        print('No Data available on Entrackr')\n",
    "\n",
    "searches = input(\"Enter company name/s(comma seperated):\")\n",
    "lst = searches.split(',')\n",
    "opt = Options()\n",
    "opt.add_argument(\"--incognito\")\n",
    "opt.add_argument('--disable-notifications')\n",
    "driver = uc.Chrome(options=opt)\n",
    "driver.maximize_window()\n",
    "action = ActionChains(driver)\n",
    "\n",
    "email = 'd20262054@gmail.com'\n",
    "p = 'Dummysamplepassword'\n",
    "\n",
    "email2 = 'd3280993@gmail.com'\n",
    "p2 = 'Dummysamplepassword2'\n",
    "\n",
    "GLOB_df = pd.DataFrame()\n",
    "\n",
    "for s in lst:\n",
    "\n",
    "    FB = Facebook_scrape()\n",
    "    time.sleep(2)\n",
    "    I = Indeed_scrape()\n",
    "    time.sleep(2)\n",
    "    # T = Twitter_scrape()\n",
    "    # time.sleep(2)\n",
    "    TOI = TOI_scrape()\n",
    "    time.sleep(2)\n",
    "    # GD = Glassdoor_scrape()\n",
    "    # time.sleep(2)\n",
    "    AB = AmbitionBox_scrape()\n",
    "    time.sleep(2)\n",
    "    EN = Entrackr_scrape()\n",
    "    time.sleep(2)\n",
    "    L = LinkedIn_scrape()\n",
    "    time.sleep(2)\n",
    "\n",
    "    final_df = reduce(lambda left, right: pd.merge(left,right,on=['Reviews','Date','Source'], how='outer'), [L])\n",
    "    final_df = final_df.merge(AB ,how ='outer', on =['Reviews','Date','Source'])\n",
    "    final_df = final_df.dropna(subset=['Reviews'])\n",
    "\n",
    "    combined_dataframe_dhruv = pd.concat([FB,TOI,df_main_entrackr,df_main_indeed], ignore_index=True)\n",
    "    combined_dataframe_dhruv = combined_dataframe_dhruv.reset_index(drop=True)\n",
    "\n",
    "    combined_dataframe = pd.concat([final_df, combined_dataframe_dhruv], ignore_index=True)\n",
    "    combined_dataframe = combined_dataframe.reset_index(drop=True)\n",
    "    combined_dataframe['Date']= pd.to_datetime(combined_dataframe['Date'] , errors = 'coerce')\n",
    "    combined_dataframe['Date'] = combined_dataframe['Date'].dt.date\n",
    "\n",
    "    GLOB_df = pd.concat([GLOB_df, combined_dataframe], ignore_index=True)\n",
    "\n",
    "driver.quit()                   #closing the  browser after extracting all the data\n",
    "\n",
    "\n",
    "GLOB_df.to_csv(\"FINAL DATA.csv\", index = False)\n",
    "print('All Data Combined Together Successfully!')\n",
    "print('\\nNext Process: Analyzing Sentiments\\n')\n",
    "\n",
    "def sentiment_analyzer():\n",
    "    print('\\nRunning Sentiment Analysis\\n')\n",
    "    import string\n",
    "    common_words = ['the','is','a','for','as','can','to','am','i','and','this','of','you','are','very','they','us','will','at','on','be','who','any','s','so','our','do']     \n",
    "    def cleaning_tokenizing(review):\n",
    "        if isinstance(review, float):\n",
    "            return \"\"  # Return an empty string if the value is a float\n",
    "        nopunc = [char for char in review if char not in string.punctuation]\n",
    "        nopunc = ''.join(nopunc)\n",
    "        nopunc = [word for word in nopunc.split() if word.lower() not in common_words]\n",
    "        nopunc = ' '.join(nopunc)\n",
    "        return nopunc\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    from textblob import TextBlob\n",
    "    nltk.download('vader_lexicon')\n",
    "    nltk.download('punkt')\n",
    "    df_main = pd.read_csv(\"FINAL DATA.csv\")\n",
    "    df_main['Text'] = df_main['Reviews'].apply(cleaning_tokenizing)\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def calc_sentiment(txt):\n",
    "        if pd.isnull(txt):\n",
    "            return TextBlob(\"\").sentiment\n",
    "        else:\n",
    "            return TextBlob(str(txt)).sentiment\n",
    "    def calc_sentiment_analyse(txt):\n",
    "        if pd.isnull(txt):\n",
    "            return sia.polarity_scores(\"\")\n",
    "        else:\n",
    "            return sia.polarity_scores(txt)\n",
    "\n",
    "    df_main['Sentiment'] = df_main['Text'].astype(str).apply(calc_sentiment)\n",
    "    df_main['Sentiment Score'] = df_main['Text'].apply(calc_sentiment_analyse)\n",
    "    df_main['Compound_score'] = None\n",
    "    df_main['Compound_score_sentiment'] = None\n",
    "\n",
    "    for i in range(0, len(df_main)): \n",
    "        df_main['Compound_score'][i] = df_main['Sentiment Score'][i]['compound']\n",
    "\n",
    "        if (df_main['Sentiment Score'][i]['compound'] <= -0.05):\n",
    "            df_main['Compound_score_sentiment'][i] = 'Negative'    \n",
    "        if (df_main['Sentiment Score'][i]['compound'] >= 0.05):\n",
    "            df_main['Compound_score_sentiment'][i] = 'Positive'\n",
    "        if (-0.05 <= df_main['Sentiment Score'][i]['compound'] <= 0.05):\n",
    "            df_main['Compound_score_sentiment'][i] = 'Neutral'\n",
    "\n",
    "    def score(res):\n",
    "        if res == 'Negative':\n",
    "            return 'Bad'\n",
    "        if res == 'Positive':\n",
    "            return 'Good'\n",
    "        if res == 'Neutral':\n",
    "            return 'Neutral'\n",
    "\n",
    "    df_main['Sentiment_score'] = df_main.Compound_score_sentiment.apply(score)\n",
    "\n",
    "    df_main.drop(\"Sentiment\", axis=1, inplace=True)\n",
    "    df_main.drop(\"Sentiment Score\", axis=1, inplace=True)\n",
    "    df_main.drop(\"Compound_score_sentiment\", axis=1, inplace=True)\n",
    "    df_main.drop('Reviews',axis=1,inplace=True)\n",
    "    df_main.rename(columns={'Compound_score': 'Sentiment Score', 'Sentiment_score': 'Sentiment','Text':'Reviews'}, inplace=True)\n",
    "\n",
    "    df_main.to_csv('FINAL DATA ANALYSED.csv',index=False)\n",
    "    print(\"\\nAll data has been saved in file 'FINAL DATA ANALYSED' within the same location.\\n\")\n",
    "\n",
    "sentiment_analyzer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
